--- 
bibliography: citations.bib
output:
  bookdown::pdf_book:
    toc: no
    template: null
    keep_tex: yes
    includes:
      in_header: tex/preamble.tex
      before_body: tex/doc_preface.tex
documentclass: book
classoption: oneside
fontsize: 12pt
subparagraph: yes
link-citations: yes
biblio-style: apalike
---

# Introduction

Randomized Trials are considered the gold standard in establishing the benefit of an intervention.  A review of Phase III trials funded by the National Institute of Neurological Disorders and Stroke systematically found the overall societal benefit of their trials to be much greater than the total costs [@Johnston:2006ju]. Societal costs that were that of trials ending pre-maturely -- before reaching a clear clinical conclusion when it could have otherwise continued.  While the overall societal benefit is great for randomized clinical trials, the face-value cost may be prohibitive for starting a trial.  Among agents approved by the Food and Drug Administration between 2015-2016, the median enrollment was 488 patients (IQR, 230 â€“ 740) with a median cost per patient of $41,117 (IQR, $31,802 - $82,362) [@Moore:wc].  The long-ago rally cry endures that: "Reducing the costs of trials is absolutely crucial for the public good" [@Collier:2009du].  Regardless of how financially well-endowed is a clinical trial, all investigators seek to reach a clear clinical conclusion while minimizing burderns and resources. From regulatory bodies, to industry, to academia, all can agree on the need to run an efficient and efficacious trial.

In this body of work, we develop new methods that increase the efficiency and balance of clinical trials and the ability to follow studies until reaching clear clinical conclusions.  The developments include novel extensions to Sequential Matched Randomization and adaptive monitoring of clinical trials using the Second Generation p-value.  While the context focuses on randomized trials, both contain insights that extend into efficient and efficacious use of observational studies.

Randomization removes systematic confounding and, for the Frequentist, further provides the foundation of accurate estimates of uncertainty in estimating treatment effects.  While the randomization space provides these overall benefits, any single instance of randomization includes at least some degree of imbalance on baseline covariates.  To reduce the risk of imbalances, trialists often turn to Stratified Block Randomization, which allocates treatments within categorical profiles (strata) of select baseline covariates.  It is easy to implement yet is limited to balancing on categorical, or categorized, baseline covariates.  The number of strata increases multiplicatively for each baseline covariate used to create strata.

In Chapter 1, we extend Sequential Matched Randomization which has already been shown to provide greater efficiency and balance than Stratified Block Randomization.  Matched Randomization is a refined form of Stratified Randomization which finds the set of patients (or clusters) which collectively are most similar to each other based upon a distance matrix.  Randomization occurs within each pair.  As patients enter a study sequentially, some optimality is lost in determining the best pairing of patients.  Our extensions allow for (1) dynamically updating an empirically estimated matching threshold throughout the study and (2) breaking matches when a better match enters the study.  These extensions nearly regain the optimality lost when patients all known and matched prior to randomization.  We show through the REACH clinical trial case-study that randomization-based inference under our method can achieve nearly the same efficiency as a fully-adjusted linear model.  And we are able to increase overall balance of baseline covariates which is important for study that which to explore subgroup analyses for personalizing medicine.  

While chapter one makes a case for our method, it also provides insight to other covariate-adjusted randomization schemes and observational studies. We note that minimization schemes vary in allowing a degree of randomization.  Some provide little randomization while other have embedded an element of randomization.  To the extent that randomization is included with minimization, it is a powerful contender for increasing efficiency and balance of baseline covariates in trials.  As for the balance of baseline covariates, the case study sheds insights on how overall balancing can be influenced by the distribution of each baseline covariate.

Randomization itself comes with a great degree of variability that can affect the sample size assumptions made when designing a clinical trial.  Traditional sample size estimates are often based upon a minimal size to detect an effect to be statistically significant.  However, achieving statistical significance alone does not imply a novel target is beneficial to the scientific community.  

In Chapter Two we introduce an adaptive monitoring design grounded on following studies until either ruling out effects deemed trivial to the null hypothesis or until ruling out a highly actionable effect that would change practice.  These effects are ruled out making use of the Second Generation p-value which draws inference upon interval hypotheses.  Under any inferential paradigm, one calculates the overlap between an estimated interval and the interval hypothesis.  Complete overlap is evidence for the hypothesis, zero overlap is evidence for the alternative, and partial overlap indicates an inconclusive study.

Monitoring a study until ruling out trivial or highly actionable effects endows the study with clinical relevance and many statistically beneficial properties.  False discoveries most commonly occur when an effect is close to the null hypothesis (the corresponding p-value is barely less than alpha).  Following a study until ruling out trivial effects reduces the false discovery probability by eliminating those effects estimated to be close to the point null.  We compare our adaptive monitoring design with monitoring with posterior probabilities which may similarly follow studies until ruling out the set of trivial or highly actionable effects.

To improve operational characteristics, the design waits until a period of time before applying monitoring rules and requires affirming alerts to stop a trial.  The wait time reduces errors which may occur before the estimated effect and interval have sufficiently stabilized.  And, the affirmation step reduces bias inherent in all adaptive monitoring designs.  

The operating characteristics of these adaptive monitoring designs require simulation, which may be a barrier to implementation.  We provide, in Chapter Three, an R package, sgpvAM, which simulates adaptive monitoring with the second generation p-value.  The function is versatile to allow the user to provide their own generated MCMC replicates with intervals or to generate MCMC replicates with effects and outcomes under any random distribution.  The output allows the user to address practical considerations such as the probability of a study ending by a certain number of observations and the probability of study being inconconclusive.  An inconclusive study can occur when outcomes are not observed immediately.  Observed outcomes may suggest stopping for ruling out trivial or highly actionable effects, yet the study may not rule out the same effects after observing the remaining outcomes.

In this chapter, we conduct extensive simulations to then make practical recommendations.  The Type I error is minimized, and less than 0.05, when waiting until the interval width has the length equal to the absolute midpoint between the trivial and highly actionable effects.  This holds true even when allowing an unrestricted, infinite sample size.  Though not investigated in this work, this open the door in a principled manner for restarting trials or continuing to gather observational data as it accumlates.  In many trials, observations are not immediately observed relative to enrollment.  By increasing the number of steps required to affirm an alert, one is able to substantively reduce the risk of an inconclusive study when outcomes are not immediately observed relative to enrollment.

This body of work lays a foundation for many future developments.  ... 